---
title: HashMap源码分析
date: 2016-12-04 11:46:58
tags: [JDK,Java]
toc: true
---

## HashMap简介

本文针对`HashMap`的源码分析基于`JDK 7`，`JDK 8`在`HashMap`的实现上有着较大幅度的改进和优化，这部分优化我将另起一篇来阐述。另外，本文仅分析`HashMap`众多方法中最常用的方法，其余方法有需要时再研究 :smile:。

`HashMap`的继承关系如下。

```java
public class HashMap<K,V>
    extends AbstractMap<K,V>
    implements Map<K,V>, Cloneable, Serializable
```

`HashMap`继承自`AbstractMap`，同时实现了`Map`、`Cloneable`和`Serializable`接口。因此，`HashMap`可以被克隆，并支持序列化。另外，`HashMap`是一个非线程安全的，因此适合运用在单线程环境下。如果是在多线程环境，可以通过`Collections`的静态方法`synchronizedMap`获得线程安全的`HashMap`，如下代码所示。

```java
Map<String, String> map = Collections.synchronizedMap(new HashMap<String, String>());
```

## 存储结构

针对每个键值对，`HashMap`使用内部类`Entry`来存储，`Entry`核心代码如下。

```java
static class Entry<K, V> implements Map.Entry<K, V> {
    final K key;
    V value;
    Entry<K, V> next;
    final int hash;
  
    Entry(int h, K k, V v, Entry<K,V> n) {
        value = v;
        next = n;
        key = k;
        hash = h;
    }
}
```

从整体上看，`HashMap`底层的存储结构是基于数组和链表实现的。对于每一个要存入`HashMap`的键值对（`Key-Value Pair`），通过计算`Key`的`hash`值来决定存入哪个数组单元（`bucket`），为了处理`hash`冲突，每个数组单元实际上是一条`Entry`单链表的头结点，其后引申出一条单链表。`HashMap`的存储结构如下图所示。

![Alt text](/img/2016-12-04-Image 1.png)

## 关键属性

`HashMap`定义了几个关键属性，对应的源码如下。

```java
static final int DEFAULT_INITIAL_CAPACITY = 16;
static final int MAXIMUM_CAPACITY = 1 << 30;
static final float DEFAULT_LOAD_FACTOR = 0.75f;
transient Entry[] table;
transient int size;
int threshold;
final float loadFactor;
```

* `DEFAULT_INITIAL_CAPACITY`代表`HashMap`槽（`bucket`）的默认容量，且该容量必须为`2`的幂，具体原因会在下文解释。
* `MAXIMUM_CAPACITY代表HashMap`槽（`bucket`）的最大容量，如果传入的容量大于`1 << 30`，那么实际容量会被`MAXIMUM_CAPACITY`替换。
* `DEFAULT_LOAD_FACTOR`是默认的加载因子，用于计算`HashMap`扩容的`threshold`，当`HashMap`的实际元素容量达到总容量的`threshold`时，对`HashMap`进行扩容。
* `table`是存储`Entry`的数组，每个`Entry`是一条单链表的头结点。
* `size`代表`HashMap`键值对的数量。
* `threshold`是`HashMap`决定是否执行执行扩容操作的阈值，`threshold  = capacity * load factor`。
* `loadFactor`表示`HashMap`实际加载因子，通过构造方法传入。若未指定，`loadFactor`等于`DEFAULT_LOAD_FACTOR`。

需要进一步解释的是`loadFactor`属性，`loadFactor`描述了`HashMap`发生扩容时的填充程度。如果`loadFactor`设置过大，意味着在`HashMap`扩容前发生`hash`冲突的机会越大，因此单链表的长度也就会越长，那么在执行查找操作时，会由于单链表长度过长导致查找的效率降低。如果`loadFactor`设置过小，那么`HashMap`的空间利用率会降低，导致`HashMap`在很多空间都没有被利用的情况下便开始扩容。

## 构造方法

`HashMap`定义了四个构造方法，源码如下。

```java
    public HashMap(int initialCapacity, float loadFactor) {
        if (initialCapacity < 0)
            throw new IllegalArgumentException("Illegal initial capacity: " +
                                               initialCapacity);
        if (initialCapacity > MAXIMUM_CAPACITY)
            initialCapacity = MAXIMUM_CAPACITY;
        if (loadFactor <= 0 || Float.isNaN(loadFactor))
            throw new IllegalArgumentException("Illegal load factor: " +
                                               loadFactor);

        // Find a power of 2 >= initialCapacity
        int capacity = 1;
        while (capacity < initialCapacity)
            capacity <<= 1;

        this.loadFactor = loadFactor;
        threshold = (int)(capacity * loadFactor);
        table = new Entry[capacity];
        init();
    }

    public HashMap(int initialCapacity) {
        this(initialCapacity, DEFAULT_LOAD_FACTOR);
    }

    public HashMap() {
        this.loadFactor = DEFAULT_LOAD_FACTOR;
        threshold = (int)(DEFAULT_INITIAL_CAPACITY * DEFAULT_LOAD_FACTOR);
        table = new Entry[DEFAULT_INITIAL_CAPACITY];
        init(); // 在源码中，init方法体不执行任何操作。
    }

    public HashMap(Map<? extends K, ? extends V> m) {
        this(Math.max((int) (m.size() / DEFAULT_LOAD_FACTOR) + 1,
                      DEFAULT_INITIAL_CAPACITY), DEFAULT_LOAD_FACTOR);
        putAllForCreate(m);
    }
```

当调用`HashMap`默认构造方法时，`HashMap`对象的属性均会被设置为默认值，包括设置加载因子（`DEFAULT_LOAD_FACTOR`）、扩容阈值（`threshold`）和`table`的初始大小。

如果在创建`HashMap`对象时指定了`bucket`容量`initialCapacity`，通过源码我们可以看出在初始化对象时不一定会直接使用`initialCapacity`，而是选取满足小于等于`initialCapacity`前提条件下最大的且是`2`的幂的一个值作为实际`bucket`的大小。

如果向构造方法传递的参数是一个`Map`对象`m`，那么`putAllForCreate`方法会重新散列`m`中的每个元素，将它们存入相应的`bucket`中。`putAllForCreate`方法及其调用的相关方法如下。

```java
    private void putForCreate(K key, V value) {
        int hash = (key == null) ? 0 : hash(key.hashCode());
        int i = indexFor(hash, table.length);

        for (Entry<K,V> e = table[i]; e != null; e = e.next) {
            Object k;
            if (e.hash == hash &&
                ((k = e.key) == key || (key != null && key.equals(k)))) {
                e.value = value;
                return;
            }
        }

        createEntry(hash, key, value, i);
    }

    private void putAllForCreate(Map<? extends K, ? extends V> m) {
        for (Map.Entry<? extends K, ? extends V> e : m.entrySet())
            putForCreate(e.getKey(), e.getValue());
    }

    static int indexFor(int h, int length) {
        return h & (length-1);
    }

    void createEntry(int hash, K key, V value, int bucketIndex) {
        Entry<K,V> e = table[bucketIndex];
        table[bucketIndex] = new Entry<>(hash, key, value, e);
        size++;
    }
```

`putAllForCreate`方法遍历每一个键值对`e`，通过`putForCreat`方法将`e`散列到对应的`bucket`中。`putForCreate`方法调用`indexFor`来确定键值对散列的`bucket`的位置。`indexFor`通过`h & (length-1)`返回`bucket`的位置，接着遍历对应的单链表来决定是更新操作还是插入操作。

我们需要关注的地方是`indexFor`为什么通过计算`h & (length-1)`来获得`bucket`的位置，而不是通过计算`h % length`？

实际上，在`HashMap`中，`h & (length-1) == h % length`，但是需要一个前提：`length`必须满足是`2`的幂。这也正是在解释`DEFAULT_INITIAL_CAPACITY`和`HashMap`构造方法时强调的`HashMap`的`bucket`容量必须是`2`的幂。当`length`是`2`的幂，那么`length`的二进制数可以表示为`1000...000`，因此`length - 1`的二进制数为`0111...111`，当`h`与`length - 1`位与时，除了`h`的最高位的被修改为`0`，其余位均保持不变，这也正是实现了`h % length`的效果。只是相比于`h % length`，`h & (length-1)`的效率会更高。

`HashMap`的`bucket`容量必须为`2`的幂的另一个重要原因是一旦满足此条件，那么`length`即为偶数，`length - 1`便为奇数，所以`length - 1`的最后一位必为`1`。因此，`h & (length - 1)`得到的值既可能是奇数，也可能是偶数，这确保了散列的均匀性。如果`length - 1`是偶数，那么`h & (length - 1)`得到的值必为偶数，那么`HashMap`的空间便浪费了一半。

## 存取方法

我们分析`HashMap`使用频率最高的两个方法`get`方法和`put`方法，源码如下。

```java
    public V get(Object key) {
        if (key == null)
            return getForNullKey();
        int hash = hash(key.hashCode());
        for (Entry<K,V> e = table[indexFor(hash, table.length)];
             e != null;
             e = e.next) {
            Object k;
            if (e.hash == hash && ((k = e.key) == key || key.equals(k)))
                return e.value;
        }
        return null;
    }

    public V put(K key, V value) {
        if (key == null)
            return putForNullKey(value);
        int hash = hash(key.hashCode());
        int i = indexFor(hash, table.length);
        for (Entry<K,V> e = table[i]; e != null; e = e.next) {
            Object k;
            if (e.hash == hash && ((k = e.key) == key || key.equals(k))) {
                V oldValue = e.value;
                e.value = value;
                e.recordAccess(this);
                return oldValue;
            }
        }

        modCount++;
        addEntry(hash, key, value, i);
        return null;
    }
```

从`HashMap`获取`get`元素时，先计算`Key`的`hash`值，定位到数组中对应的`bucket`，然后开始遍历`Entry`单链表，直到找到需要的元素，否则返回`null`。

当我们向`HashMap`中`put`新的键值对时，`HashMap`首先检查`Key`是否等于`null`，若为`null`，则执行`putForNullKey`方法，`putForNullKey`方法对应的源码如下。

```java
    private V putForNullKey(V value) {
        for (Entry<K,V> e = table[0]; e != null; e = e.next) {
            if (e.key == null) {
                V oldValue = e.value;
                e.value = value;
                e.recordAccess(this); // 不做任何操作
                return oldValue;
            }
        }
        modCount++;
        addEntry(0, null, value, 0);
        return null;
    }
```

如果`Key`等于`null`，那么就将该键值对添加到`table[0]`的位置，同时，遍历`table[0]`处的单链表并将链表中所有节点的值都覆盖为新传递进来的键值对的值。因此，该位置永远只有一个值。

如果`Key`不等于`null`，那么通过`indexFor`定位到`bucket`，然后遍历单链表，如果存在`Key`相等的键值对，就用新值覆盖旧值，并返回旧值。如果在单链表中没有找到对应的`Key`，那么调用`addEntry`方法创建新的`Entry`节点至单链表（作为头节点）。`addEntry`及关联方法源码如下。

```java
    void addEntry(int hash, K key, V value, int bucketIndex) {
        Entry<K,V> e = table[bucketIndex];
        table[bucketIndex] = new Entry<>(hash, key, value, e);
        if (size++ >= threshold)
            resize(2 * table.length);
    }

    void resize(int newCapacity) {
        Entry[] oldTable = table;
        int oldCapacity = oldTable.length;
        if (oldCapacity == MAXIMUM_CAPACITY) {
            threshold = Integer.MAX_VALUE;
            return;
        }

        Entry[] newTable = new Entry[newCapacity];
        transfer(newTable);
        table = newTable;
        threshold = (int)(newCapacity * loadFactor);
    }

    void transfer(Entry[] newTable) {
        Entry[] src = table;
        int newCapacity = newTable.length;
        for (int j = 0; j < src.length; j++) {
            Entry<K,V> e = src[j];
            if (e != null) {
                src[j] = null;
                do {
                    Entry<K,V> next = e.next;
                    int i = indexFor(e.hash, newCapacity);
                    e.next = newTable[i];
                    newTable[i] = e;
                    e = next;
                } while (e != null);
            }
        }
    }
```

当`addEntry`把新增键值对插入单链表后，会判断是否需要扩容，即判断当前`HashMap`的元素的个数是否大于`threshold`。若需要扩容，那么调用`resize`方法进行`2`倍扩容。`resize`方法会在内部调用`transfer`方法，`transfer`方法遍历旧数组及单链表，并将每个键值对重新散列，可以意识到，这整个`rehash`的开销相当大。

## 线程安全

关于线程安全，我们想要知道的是`HashMap`在什么情况下会发生线程不安全的情况？实际上，在上文分析`put`方法时，当`HashMap`的容量超过了`threshold`时，便执行`resize`操作，`resize`就存在线程不安全的问题。

关于`resize`哪儿不安全，我推荐左耳朵耗子写的 [疫苗：Java HashMap的死循环](http://coolshell.cn/articles/9606.html/)，这篇文章图文并茂的解释了在`rehash`过程中出现线程不安全问题的根源。

## HashMap VS HashTable

`HashTable`和`HashMap`底层采用相同的存储结构，在很多方法的实现上二者的思路基本一致。最主要的区别主要有两点。

* `HashTable`实现了所谓的线程安全，在`HashTable`很多方法上都加上了`synchronized`。
* 在`HashMap`的分析中，我们发现当我们新增键值对时，`HashMap`是允许`Key`和`Value`均为`null`。但是`HashTable`不允许`Key`或`Value`为`null`，关于这一点我们可以通过查看`HashTable`源码得知。

```java
    public synchronized V put(K key, V value) {
        // Make sure the value is not null
        if (value == null) { // 若value为空则抛出NullPointerException。
            throw new NullPointerException();
        }

        // Makes sure the key is not already in the hashtable.
        Entry<?,?> tab[] = table;
        int hash = key.hashCode(); // 若key为空则抛出NullPointerException。
        int index = (hash & 0x7FFFFFFF) % tab.length;
        @SuppressWarnings("unchecked")
        Entry<K,V> entry = (Entry<K,V>)tab[index];
        for(; entry != null ; entry = entry.next) {
            if ((entry.hash == hash) && entry.key.equals(key)) {
                V old = entry.value;
                entry.value = value;
                return old;
            }
        }

        addEntry(hash, key, value, index);
        return null;
    }
```
