


开始学习大名鼎鼎的[MIT 6.824: Distributed Systems](http://nil.csail.mit.edu/6.824/2016/index.html)课程，我跟的是`2016`年的课程，课程的主要内容是读`Paper`和做`Lab`，使用的语言为`Go`。五一假期期间我基本做完了[Lab 1](http://nil.csail.mit.edu/6.824/2016/labs/lab-1.html)，感觉难度还是相当大的。本篇文章是我对`Lab 1`的一个总结。

<!--more-->

## MapReduce

每次读[MapReduce](https://static.googleusercontent.com/media/research.google.com/zh-CN//archive/mapreduce-osdi04.pdf)论文，都会有新的收获，也自知还有理解不到位的地方。

### Execution Overview

* 输入数据被划分为`M`个分片，由`map worker`产生的中间`key-value pairs`被划分为`R`个分片。其中`M`的大小取决于`GFS`块的大小，`R`取决于`reduce worker`的个数。所以，整个`MapReduce Job`包括`M`个`map task`和`R`个`reduce task`。
* 在`map`阶段，`map worker`把输出的中间`key-value pairs`分割成`R`个`region`，注意`pairs`首先会存储在缓冲区中，然后定期的写入本地磁盘。`pairs`在`map worker`上的位置信息会发送给`master`，由`master`通知`reduce worker`数据读取位置信息。
* 在`reduce`阶段，当`reduce worker`获取了输入数据的位置信息后，通过`RPC`读取数据。当`reduce worker`获取了所有的相关数据之后，会对它们进行一次排序，排序的目的在于不同`key`的`pairs`会汇聚到同一个`reduce worker`。
* 当所有的`task`完成后，一共会产生`R`个输出文件，每个`reduce worker`对应一个。一般来说没有必要将这`R`个文件合并成一个文件，因为这`R`个文件往往会作为下一个`MapReduce Job`的输入数据。

### Master Data Structures

* `master`会记录所有`map task`和`reduce task`的状态(`idle`，`in-progress`和`completed`)信息。同时，`master`还会保存那些处于`non-idle`状态的`task`所关联的`worker`的信息。
* 对于每个状态为`completed`的`map task`，`master`会保存该`map task`对应的`R`个`region`的位置信息和大小信息。

### Fault Tolerance

#### Worker Failure

* `master`通过周期性的`ping`所有的`worker`来确认`worker`是否可用，如果某个`worker`崩溃了，那么在该`worker`上完成的所有`map task`都会回退到`idle`状态，因此这些`task`会被重新调度到可用的`worker`上。同理，如果处于`in-progress`的`map task`或`reduce task`所在的`worker`崩溃了，那么这些`task`也会被重新调度到可用的`worker`上重新执行。
* 如果某个`worker`崩溃了，其上处于`completed`状态的`map task`需要重新被调度执行，这是因为`map task`的输出数据是存储于`local disk`。相反，如果某个`worker`崩溃了，其上处于`completed`状态的`reduce task`不需要重新被调度执行，因为`reduce task`的输出数据是存储在`global file system`上的。
* 当`reduce worker`在执行`reduce task`时，如果某个`map task`对应的`worker`由`worker A`切换到了`worker B`(可能是因为`worker A`崩溃了)，那么`master`会通知该`reduce worker`从`worker B`读取数据。

#### Master Failure

* `master`通过定期的做`checkpoints`来保证`master`的容错性。
* 由于`master`只有一个，所以可以认为`master`出故障的概率很小。如果真的出故障了，那么根据需要重新执行`MapReduce Job`。

## Lab

`Lab 1`包括`4`个`Part`，`Part 1 & Part 2`实现`Sequential MapReduce`，`Part 3 & Part 4`实现`Distributed MapReduce`，并且要解决`worker failure`。同时，`Lab 1`提供了`MapReduce`的整个框架，并实现了与核心内容无关的代码。在完成各个`Part`之前，我们需要理解`Lab 1`实现的两个版本的`MapReduce`框架的设计思路。

* `master`和`worker`以`goroutine`的形式存在，当`worker`可用时，以`RPC`的方式向`master`注册，`master`通过调用`schedule()`来实现`task`的调度。同时，在`Distributed MapReduce`中`schedule()`还需要处理`worker failure`。`schedule()`将会以参数的形式存在于`Sequential/Distributed MapReduce`。
* 论文中`map task`以块作为分割，而`Lab`中的`map task`将以文件作为分割。
* 在`map`阶段，对于每个`map task`(一个输入文件)，`master`会至少调用一次`doMap()`。同理在`reduce`阶段，`master`对每个`reduce task`至少调用一次`doReduce()`。在`Sequential MapReduce`中，`Sequential()`[`master.go`]的核心参数`schedule()`遍历了`file slice`，根据当前所处的`phase`对每个`task`调用`doMap()`或`doReduce()`。在`Distributed MapReduce`中，`Distributed()`[`master.go`]的核心参数`schedule()`需要我们去实现(`Part 3 & Part 4`)。
* 在完成了所有的`map task`和`reduce task`之后，`master`会调用`merge()`对`reduce worker`的输出文件进行合并。
* 最后`master`向每个`worker`发送`Shutdown RPC`，然后关闭整个应用。

理解完整个`Lab`的框架，下面谈谈每个`Part`需要注意的地方。

### Part 1: Map/Reduce input and output

`Part 1`主要的内容是完成`doMap()`和`doReduce()`，在编码前一定要仔细阅读注释给我们提供的信息。

* `doMap()`读取输入文件(`inFile`)的内容，通过`mapF`将其转化为`[]KeyValue`。对于每个`inFile`，`doMap`将产生`nReduce`(对应于论文中的`R`)个输出文件。同时，数据以`JSON`的格式存储至输出文件中。
* `doReduce()`从`nMap`(对应于论文中的`M`)个`map worker`读取输入数据(`[]KeyValue`)，注意输入数据的格式为`JSON`。根据`key`构建出`map[key]values`数据结构，对每个`key`(`string`)和对应的`values`(`[]string`)调用`reduceF()`。`doReduce()`产生一个输出文件。

### Part 2: Single-worker word count

`Part 2`实现经典的`WordCount`，需要我们实现`mapF`和`reduceF`中的逻辑。需要注意的是要按照`Lab`提供的切词规范(`determined by unicode.IsLetter`)去分词，而不要想当然的用空格作为分隔符。

### Part 3: Distributing MapReduce tasks

`Part 3`着手实现`Distributed MapReduce`，为了模拟真正的分布式环境，`master`和`worker`之间的通信和同步仅通过`RPC`和`channel`来实现。`Part 3`需要我们实现`schedule()`的调度逻辑。

* `Distributed MapReduce`的实现逻辑在上文已有所提及，`Sequential MapReduce`顺序调度每个`task`执行，而在`Distributed MapReduce`中，`schedule()`将`task`调度到当前可用的`worker`上去执行。
* 只有当`map/reduce`阶段所有的`task`全部完成后，`schedule()`才能返回。这一点在代码中我是通过`sync.WaitGroup`来实现。
* 在分布式环境下，`schedule()`通过`RPC`调度`worker`执行`task`，同时输入文件的信息也是作为`RPC`参数传递。
* 当前可用的`worker`存储于`master`的`registerChannel`中。如果当前没有可用的`worker`，那么`schedule()`会进入阻塞状态，直至出现新的可用的`worker`。只有当存在可用的`worker`时才允许创建`goroutine`执行`task`(这一点体现在`schedule.go`的`line 38`和`line 42`)。

### Part 4: Handling worker failures

`Part 4`需要解决`worker failure`，映射到具体的代码实现上，`work failure`是指在进行`RPC`时，`RPC`服务器返回了`call failed`信息。根据论文，我们只需要将`task`重新分配给另一个可用的`worker`，并将上次`RPC`失败的输入文件信息作为本次`RPC`的参数传递。

需要注意的是，`RPC`失败并不一定意味着`worker`崩溃了，也有可能是因为网络原因导致`worker`不可达，所以有可能导致多个`worker`在执行着相同的`task`。不过由于`task`的幂等性，这并没有什么问题，`master`记录着每个`task`对应的真正有效的`worker`。

## Implementation

`Lab 1`的代码实现在[这里](https://github.com/tinylcy/mit-6.824)。